{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqJSPKTm-3F9"
   },
   "source": [
    "# **Question:1 Use the inaugural address corpus to find the total number of words and the total number of unique words in the inaugural addresses delivered in the 21st century**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgoyfAfuBkER",
    "outputId": "b3a8e8be-60ab-436d-b65c-4510e82c8a32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in 21st-century inaugural addresses: 14093\n",
      "Total number of unique words in 21st-century inaugural addresses: 2494\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# Ensure the necessary NLTK datasets are downloaded\n",
    "nltk.download('inaugural')\n",
    "\n",
    "# Function to filter the addresses from the 21st century\n",
    "def get_21st_century_addresses():\n",
    "    # List of files in the inaugural corpus\n",
    "    files = inaugural.fileids()\n",
    "\n",
    "    # Filter for 21st century addresses (2001 and onwards)\n",
    "    century_21_files = [file for file in files if int(file.split('-')[0]) >= 2001]\n",
    "\n",
    "    return century_21_files\n",
    "\n",
    "# Get the 21st-century inaugural addresses\n",
    "century_21_files = get_21st_century_addresses()\n",
    "\n",
    "# List to store all words from the 21st century addresses\n",
    "all_words = []\n",
    "\n",
    "# Iterate through each 21st century address and extract words\n",
    "for file in century_21_files:\n",
    "    words = inaugural.words(file)\n",
    "    all_words.extend(words)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_words = len(all_words)\n",
    "\n",
    "# Calculate the total number of unique words\n",
    "unique_words = set(all_words)\n",
    "total_unique_words = len(unique_words)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total number of words in 21st-century inaugural addresses: {total_words}\")\n",
    "print(f\"Total number of unique words in 21st-century inaugural addresses: {total_unique_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTBDsOwTCDje"
   },
   "source": [
    "# **Question:2 Write a Python program to find the frequency distribution of the words \"democracy\", \"freedom\", \"liberty\", and \"equality\" in all inaugural addresses using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fr1SR80CLCz",
    "outputId": "3a6e6492-ad2a-4dd7-ecd5-375171877df6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of 'democracy': 71\n",
      "Frequency of 'freedom': 189\n",
      "Frequency of 'liberty': 123\n",
      "Frequency of 'equality': 26\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Ensure the necessary NLTK datasets are downloaded\n",
    "nltk.download('inaugural')\n",
    "\n",
    "# List of target words to track frequency for\n",
    "target_words = [\"democracy\", \"freedom\", \"liberty\", \"equality\"]\n",
    "\n",
    "# List to store all words from all inaugural addresses\n",
    "all_words = []\n",
    "\n",
    "# Iterate through all inaugural addresses\n",
    "for file in inaugural.fileids():\n",
    "    # Get the words in the current address\n",
    "    words = inaugural.words(file)\n",
    "    # Add the words to the all_words list\n",
    "    all_words.extend(words)\n",
    "\n",
    "# Convert all words to lowercase for case-insensitive matching\n",
    "all_words = [word.lower() for word in all_words]\n",
    "\n",
    "# Create a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# Find the frequency of the target words\n",
    "target_word_frequencies = {word: fdist[word.lower()] for word in target_words}\n",
    "\n",
    "# Print the frequency of each target word\n",
    "for word, frequency in target_word_frequencies.items():\n",
    "    print(f\"Frequency of '{word}': {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOCM6DZeCQcj"
   },
   "source": [
    "# **Question:3 Write a Python program to display the 5 most common words in the text of \"Sense and Sensibility\" by Jane Austen using the Gutenberg Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dB4Qka6ACax4",
    "outputId": "ab1d59a1-8dd3-4469-df9e-fdc395505b69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most common words in 'Sense and Sensibility' are:\n",
      "',': 9397\n",
      "'to': 4116\n",
      "'the': 4105\n",
      "'.': 3975\n",
      "'of': 3572\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Ensure the necessary NLTK datasets are downloaded\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Load the text of \"Sense and Sensibility\" by Jane Austen from the Gutenberg corpus\n",
    "text = gutenberg.words('austen-sense.txt')\n",
    "\n",
    "# Convert all words to lowercase for case-insensitive counting\n",
    "text = [word.lower() for word in text]\n",
    "\n",
    "# Create a frequency distribution of the words\n",
    "fdist = FreqDist(text)\n",
    "\n",
    "# Display the 5 most common words\n",
    "most_common_words = fdist.most_common(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"The 5 most common words in 'Sense and Sensibility' are:\")\n",
    "for word, frequency in most_common_words:\n",
    "    print(f\"'{word}': {frequency}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
