{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMKLVrO2-6SR"
   },
   "source": [
    "# ***Question No: 1 -->\tWrite a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGBBBZP8-lz6",
    "outputId": "2a7bbe16-4196-4e35-801e-3bf1c8345da0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# Input sentence\n",
    "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# Tokenize and POS tag the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition (NER)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Print named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gW6U_JbA_ClF",
    "outputId": "e60164da-ddbb-4643-d3e8-c00c1b5ae805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION Inc./NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLKXFlLy_772"
   },
   "source": [
    "# ***Question No: 2 --> Using NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRTJZprj_ij0",
    "outputId": "6576e1cd-3281-435d-aa65-32ddd329a3d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_named_entities(sentences):\n",
    "    named_entities_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)  # Tokenize sentence\n",
    "        pos_tags = pos_tag(tokens)  # Part-of-speech tagging\n",
    "        chunked_tree = ne_chunk(pos_tags)  # Named Entity Recognition\n",
    "\n",
    "        entities = []\n",
    "        for subtree in chunked_tree:\n",
    "            if isinstance(subtree, nltk.Tree):  # Check if it's a named entity\n",
    "                entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "                entity_type = subtree.label()  # Entity type (e.g., PERSON, ORGANIZATION, GPE)\n",
    "                entities.append((entity_name, entity_type))\n",
    "\n",
    "        named_entities_list.append(entities)\n",
    "\n",
    "    return named_entities_list\n",
    "\n",
    "# Example usage\n",
    "sentences = [\n",
    "    \"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
    "    \"Elon Musk founded SpaceX in 2002 in California.\",\n",
    "    \"Google was founded by Larry Page and Sergey Brin.\"\n",
    "]\n",
    "\n",
    "result = extract_named_entities(sentences)\n",
    "\n",
    "# Print named entities for each sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnS_jf5t_utm",
    "outputId": "4eb2b04f-61e0-4aec-e69e-3df1fd66557c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('Apple', 'PERSON'), ('Inc.', 'ORGANIZATION')]\n",
      "Sentence 2: [('Elon', 'PERSON'), ('Musk', 'PERSON'), ('SpaceX', 'ORGANIZATION'), ('California', 'GPE')]\n",
      "Sentence 3: [('Google', 'PERSON'), ('Larry Page', 'PERSON'), ('Sergey Brin', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "for i, entities in enumerate(result):\n",
    "    print(f\"Sentence {i+1}: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IUWbw2tAEeN"
   },
   "source": [
    "# ***Question No: 3 --> Write a Python program that uses NLTK to extract and display all noun phrases from a given text.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwEXItCR_w5V",
    "outputId": "aecb2b06-5b5e-412d-f531-bb96e749ae4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_noun_phrases(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    pos_tags = pos_tag(tokens)  # Part-of-speech tagging\n",
    "\n",
    "    # Define a simple grammar for noun phrases (NP)\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"  # Determiner (optional) + Adjective (optional) + Noun(s)\n",
    "\n",
    "    # Create a parser\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    chunk_tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "    noun_phrases = []\n",
    "    for subtree in chunk_tree:\n",
    "        if isinstance(subtree, Tree) and subtree.label() == \"NP\":\n",
    "            noun_phrase = \" \".join(word for word, pos in subtree.leaves())\n",
    "            noun_phrases.append(noun_phrase)\n",
    "\n",
    "    return noun_phrases\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog. Apple Inc. is a big company.\"\n",
    "\n",
    "noun_phrases = extract_noun_phrases(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvKcPRDyAJll",
    "outputId": "0797902c-d1ad-4413-b1f7-cd4489fcc070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases: ['The quick brown fox', 'the lazy dog', 'Apple Inc.', 'a big company']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun Phrases:\", noun_phrases)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
